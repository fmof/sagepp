// Header for word-based topic models

#ifndef ISAGE_WTM_VARIATIONAL_H_
#define ISAGE_WTM_VARIATIONAL_H_

#include "dmc.hpp"
#include "mathops.hpp"
#include "util.hpp"
#include "wtm.hpp"

#include <fstream>
#include <iostream>
#include "stdlib.h"
#include <time.h>

// for pair
#include "map"
#include <utility>
#include <unordered_set>
#include <string>
#include <vector>

#include <fstream>
#include <boost/algorithm/string.hpp>
#include <boost/bind.hpp>
#include <boost/serialization/unordered_map.hpp>
#include <boost/serialization/vector.hpp>

#include <gsl/gsl_rng.h>

namespace isage {
  namespace wtm {
    struct UniformHyperSeedWeightedInitializer {
    private:
      int nt_;
      double inv_nt_;
      double tu_;
      double t_;
    public:
      UniformHyperSeedWeightedInitializer(int num_topics, double topic_usage_weight, double topic_weight) : nt_(num_topics), tu_(topic_usage_weight), t_(topic_weight) {
	inv_nt_ = 1.0/(double)nt_;
      }
      std::vector<double> assignment() {
	std::vector<double> vec(nt_, inv_nt_);
	mathops::add_uniform_noise(&vec, -inv_nt_, inv_nt_);
	double norm = isage::util::sum(vec);
	isage::util::scalar_product(1.0/norm, &vec);
	return vec;
      }
      std::vector<double> topic(const std::vector<double>& hyper) {
	const double d = t_/(double)hyper.size();
	std::vector<double> vec(hyper.size(), d);
	isage::util::sum_in_first(&vec, hyper);
	mathops::add_uniform_noise(&vec, -d, d);
	return vec;
      }
      template <typename Corpus, typename Vocab>
      std::vector<double> topic(const std::vector<double>& hyper,
                                const Corpus* corpus,
                                const Vocab* vocab) {
	WARN << "This version of UniformHyperSeedWeightedInitializer.topic is a dummy stub.";
	return topic(hyper);
      }
      std::vector<double> usage(const std::vector<double>& hyper, int num_words) {
	const double d = (double)num_words/(double)hyper.size();
	std::vector<double> vec(hyper.size(), d);
	isage::util::sum_in_first(&vec, hyper);
	mathops::add_uniform_noise(&vec, -d, d);
	return vec;
      }
      std::vector<double> usage(const std::vector<double>& hyper) {
	const double d = tu_/(double)hyper.size();
	std::vector<double> vec(hyper.size(), d);
	isage::util::sum_in_first(&vec, hyper);
	mathops::add_uniform_noise(&vec, -d, d);
	return vec;
      }
    };
    
    struct LDAVStrategy {
      LDAVStrategy() {
      }
      double eta_density_threshold = 1E-4;
      int num_learn_iters = 100;
      int num_e_iters = 25;
      int num_m_iters = 1;
      int hyper_update_min = 20;
      int hyper_update_iter = 5;
      int update_model_every = 5;
      int print_topics_every = 5;
      int print_topics_k = 10;
      int print_usage_every = 5;
      int em_verbosity = 1;
      bool heldout = false;
    };


    /**
     * Variational inference for word topic models.
     * This assumes that each observation is generated by a
     * categorical (discrete) distribution with a general
     * exponential family parametrization.
     */
    template <typename D, typename W, typename TopicType>
    class DiscreteVariational {
    private:
      typedef Vocabulary<W> V;
      typedef std::vector<double> ModelTopicType;
      typedef DiscreteLDA<W, ModelTopicType > M;
      int num_docs_;

      // Topics x Vocab
      std::vector< std::vector< double > > var_topic_params_;
      // Documents x Topics
      std::vector< std::vector< double > > var_topic_usage_params_;
      // This member is Dense x Sparse
      std::vector< std::vector< int > > words_in_docs_;
      // This member is Dense x Sparse
      std::vector< std::vector< int > > word_type_counts_;
      // Documents x Types in Doc x Topics
      // index this with words_in_docs_;
      std::vector< std::vector< std::vector< double > > > var_assignment_params_;

      // the model itself
      M* model_;
      Corpus<D>* corpus_;
      V* vocab_;

      int num_topics_;

      std::vector<double> word_hypers_;
      std::vector<double> usage_hypers_;


      friend class boost::serialization::access;
      // When the class Archive corresponds to an output archive, the
      // & operator is defined similar to <<.  Likewise, when the class Archive
      // is a type of input archive the & operator is defined similar to >>.
      template<class Archive>
      void serialize(Archive& ar, const unsigned int version) {
        ar & num_topics_;
        ar & vocab_;
        ar & word_hypers_;
        ar & usage_hypers_;
        ar & words_in_docs_;
        ar & var_assignment_params_;
        ar & var_topic_usage_params_;
	ar & var_topic_params_;
      } 

    public:
      DiscreteVariational<D, W, TopicType>() :  model_(NULL), corpus_(NULL) {
      }
      DiscreteVariational<D, W, TopicType>(M* model, Corpus<D>* corpus, V* vocab) :
      num_docs_(corpus->num_docs()),  
	model_(model), corpus_(corpus), vocab_(vocab), 
	num_topics_(model->num_topics()), 
        word_hypers_(model_->hyper_word()), usage_hypers_(model_->hyper_theta()) {
      }
      ~DiscreteVariational() {
      }
      void var_assignment_params(size_t i,
				 const std::vector<std::vector< double> > & info) {
	var_assignment_params_[i] = info;
      }
      void var_assignment_params(size_t i, size_t j,
				 const std::vector< double> & info) {
	var_assignment_params_[i][j] = info;
      }
      void var_topic_usage_params(size_t i, const std::vector<double>& row) {
	var_topic_usage_params_[i] = row;
      }
      const std::vector<std::vector<std::vector< double> > >& var_assignment_params() {
	return var_assignment_params_;
      }
      const std::vector<std::vector<double> > var_topic_usage_params() {
	return var_topic_usage_params_;
      }
      const std::vector<std::vector<int> >& word_type_counts() {
        return word_type_counts_;
      }
      void word_type_counts(const std::vector<std::vector<int> >& info) {
	word_type_counts_ = info;
      }
      const std::vector<std::vector<int> >& words_in_docs() {
	return words_in_docs_;
      }
      void words_in_docs(const std::vector<std::vector<int> >& info) {
	words_in_docs_ = info;
      }

      void word_hypers(const std::vector<double>& f) {
	word_hypers_ = f;
      }
      std::vector<double>& word_hypers() {
	return word_hypers_;
      }
      void usage_hypers(const std::vector<double>& f) {
	usage_hypers_ = f;
      }
      std::vector<double>& usage_hypers() {
	return usage_hypers_;
      }

      void num_topics(int nt) {
	num_topics_ = nt;
      }
      int num_topics() {
	return num_topics_;
      }

      void corpus(Corpus<D>* corpus) {
        corpus_ = corpus;
        if(num_docs_ != corpus_->num_docs()) {
          WARN << "The number of docs has changed from " << num_docs_ << " to " << corpus_->num_docs() << ". Please call reinit().";
        }
        num_docs_ = corpus_->num_docs();
      }
      void model(M* model) {
        model_ = model;
      }
      M reconstruct_model() {
        M model(this->num_topics_);
        model.hyper_theta(this->usage_hypers_);
        model.hyper_word(this->word_hypers_);
        this->update_model(&model);
        return model;
      }
      void vocab(V* vocab) {
	vocab_ = vocab;
      }
      V* vocab() {
        return vocab_;
      }

      /**
       * DiscreteVariationalInitializer must have four functions:
       */
      template <typename DiscreteVariationalInitializer>
      void init(DiscreteVariationalInitializer vi) {
	int num_words = 0;
	for(int d = 0; d < num_docs_; d++) {
	  const D& doc = (*corpus_)[d];
	  var_topic_usage_params_.push_back(vi.usage(model_->hyper_theta(), doc.num_words()));
	  const typename D::Multinomial doc_multi = doc.multinomial();
	  std::vector<int> word_types;
	  std::vector<int> wcounts;
	  for(const auto& count_pair : doc_multi) {
	    const int word = vocab_->index(count_pair.first);
	    word_types.push_back(word);
	    wcounts.push_back(count_pair.second);
	    num_words += count_pair.second;
	  }
	  words_in_docs_.push_back(word_types);
	  word_type_counts_.push_back(wcounts);
	  var_assignment_params_.push_back(std::vector<std::vector<double> >(doc_multi.size(), 
									     vi.assignment())); 
	}
        for(int t = 0; t < model_->num_topics(); t++) {
	  var_topic_params_.push_back(vi.topic(model_->hyper_word(), corpus_, vocab_));
	}
      }

      /**
       * DiscreteVariationalInitializer must have four functions:
       */
      template <typename DiscreteVariationalInitializer>
      void reinit(DiscreteVariationalInitializer vi) {
      	int num_words = 0;
      	words_in_docs_.clear();
      	word_type_counts_.clear();
      	var_topic_usage_params_.clear();
      	var_assignment_params_.clear();
      	for(int d = 0; d < num_docs_; d++) {
      	  const D& doc = (*corpus_)[d];
      	  var_topic_usage_params_.push_back(vi.usage(model_->hyper_theta()));
      	  const typename D::Multinomial doc_multi = doc.multinomial();
      	  std::vector<int> word_types;
      	  std::vector<int> wcounts;
      	  for(const auto& count_pair : doc_multi) {
      	    const int word = vocab_->index(count_pair.first);
      	    word_types.push_back(word);
      	    wcounts.push_back(count_pair.second);
      	    num_words += count_pair.second;
      	  }
      	  words_in_docs_.push_back(word_types);
      	  word_type_counts_.push_back(wcounts);
      	  var_assignment_params_.push_back(std::vector<std::vector<double> >(doc_multi.size(), 
      									     vi.assignment())); 
      	}
      }

      void update_var_assignments(int doc_index, int word,
				  const std::vector<std::vector<double> >* t_grad,
				  const std::vector<double>& u_grad,
				  const size_t& which_index) {
	var_assignment_params_[doc_index][which_index] = u_grad;
	std::vector<double>* vaptr = &(var_assignment_params_[doc_index][which_index]);
	for(size_t idx = 0; idx < num_topics_; ++idx) {
	  vaptr->operator[](idx) += t_grad->operator[](idx)[word];
	}
	double lnorm = mathops::log_sum_exp(var_assignment_params_[doc_index][which_index]);
	isage::util::sum(-1 * lnorm, vaptr);
	isage::util::exp(vaptr);
      }

      void update_usage(int doc_index) {
	const std::vector<int>& words = words_in_docs_[doc_index];
	const std::vector<std::vector<double> >& var_assign = var_assignment_params_[doc_index];
	std::vector<double> accumulator(num_topics_, 0.0);
	for(size_t i = 0; i < words.size(); ++i) {
	  const int count = word_type_counts_[doc_index][i];
          isage::util::linear_combination_in_first(&accumulator,
                                                   var_assign[i],
                                                   1.0, (double)count);
	}
	var_topic_usage_params_[doc_index] = usage_hypers_;
	isage::util::sum_in_first(&(var_topic_usage_params_[doc_index]), 
                                  accumulator);
      }

      void update_topics(int topic_index) {
	var_topic_params_[topic_index] = model_->hyper_word();
	for(size_t doc_index = 0; doc_index < num_docs_; ++doc_index){
	  const std::vector<int>& words = words_in_docs_[doc_index];
	  const std::vector<std::vector<double> >& var_assign = var_assignment_params_[doc_index];
	  std::vector<double> accumulator(vocab_->num_words(), 0.0);
	  for(size_t i = 0; i < words.size(); ++i) {
	    const int word = words[i];
	    const int count = word_type_counts_[doc_index][i];
	    accumulator[word] += ((double)count)*var_assign[i][topic_index];
	  }
	  isage::util::sum_in_first(&(var_topic_params_[topic_index]), accumulator);
	}
	DEBUG << "topic " << topic_index << " variational sums to " << isage::util::sum(var_topic_params_[topic_index]);
      }
      void update_topics() {
        for(int ti = 0; ti < num_topics_; ++ti) {	  
	  update_topics(ti);
	}
      }

      inline void get_usage_estimates(std::vector<std::vector<double> >*  usage_ptr) {
        for(const auto& use : var_topic_usage_params_) {
          const double norm = isage::util::sum(use);
          usage_ptr->push_back(isage::util::scalar_product(1.0/norm, use));
        }
      } 

      void update_model(M* m, bool heldout = false) {
	if(!heldout) {
	  std::vector< ModelTopicType > topics;
	  for(const auto& utopic : var_topic_params_) {
	    const double norm = isage::util::sum(utopic);
	    TopicType topic(isage::util::scalar_product(1.0/norm, utopic));
	    topics.push_back(topic);
	  }
	  m->prior_word(topics);
	}
        std::vector<std::vector<double> > usage;
        this->get_usage_estimates(&usage);
        m->prior_topic(usage);
      }
      void update_model(bool heldout = false) {
        update_model(model_, heldout);
      }

      inline void e_step(const LDAVStrategy& strategy) {
	std::vector<std::vector<double> > topics_grads(num_topics_,
						       std::vector<double>(vocab_->num_words()));
	const std::vector<std::vector<double> >* tgptr = &topics_grads;
        if(strategy.em_verbosity >= 1) {
          INFO << "E-step";
        }
	for(int e_iter = 0; e_iter < strategy.num_e_iters; ++e_iter) {
          if(strategy.em_verbosity >= 2) {
            INFO << "\tE-step sub-iteration number " << e_iter;
          }
	  for(size_t ti = 0; ti < num_topics_; ++ti) {
	    dmc::dirichlet::grad_log_partition_static(var_topic_params_[ti],
						      &(topics_grads[ti]));
	  }
          for(int di = 0; di < num_docs_; ++di){
            if(strategy.em_verbosity >= 3 && di % 1000 == 0) {
              INFO << "\t\tDocument " << di << " of iteration " << e_iter;
            }
            std::vector<double> usage_grad = 
              dmc::dirichlet::grad_log_partition_static(var_topic_usage_params_[di]);
	    // iterate through all word types in the document
	    // note that this is sparse
	    const std::vector<int>& words = words_in_docs_[di];
	    for(size_t i = 0; i < words.size(); ++i) {
	      const int word = words[i];
	      update_var_assignments(di, word, tgptr,
                                     usage_grad, i);
	    }
	    update_usage(di);
	  }
	}
      }

      // The basic idea here is to do a one-iteration 
      // warm-start M-step.
      void m_step(const LDAVStrategy& strategy) {
        if(strategy.em_verbosity >= 1) {
          INFO << "M-step";
        }
	for(int m_iter = 0; m_iter < strategy.num_m_iters; ++m_iter) {
          if(strategy.em_verbosity >= 2) {
            INFO << "\tM-step sub-iteration number " << m_iter;
          }
	  // first update the etas
	  update_topics();
	  // then update tau... except following the SAGE 
          // implementation, since we're using the improper
          // Jeffrey's prior, we don't have to.
	}
      }

      inline void update_hypers() {
	typedef dmc::DirichletVariationalClosure ClosureType;
	ClosureType closure;
	closure.variational_params = &var_topic_usage_params_;
	// INFO << "The closure suff. stats are: ";
	// isage::util::print_2d(var_topic_usage_params_);
	double f_init = dmc::dirichlet::hyperparameters_variational_objective(usage_hypers_, &closure);
	std::vector<double> point = dmc::dirichlet::hyperparameters_variational_nr(usage_hypers_, &closure);
	double f_end = dmc::dirichlet::hyperparameters_variational_objective(point, &closure);
	const double dist = isage::util::dist(usage_hypers_, point);
	INFO << "Hyperparameter optimization moved the alpha point " << dist << " units away";
	INFO << "Hyperparameter optimization moved the value " << (f_end - f_init) << ", from " << f_init << " to " << f_end;
	usage_hypers_ = point;
      }

      /**
       * Print out the topic assignment params as
       * doc_id [TAB] vocab_id [TAB] topic [TAB] word_count [TAB] val
       */
      void print_assignment_params(std::ostream& writer) {
	int doc_id = 0;
	writer << "doc_id\tword\ttopic\tcount\tval\n";
	for(const auto& doc_assigns : var_assignment_params_) {
	  int d_w_id = 0;
	  for(const auto& word_vec : doc_assigns) {
	    int t_id = 0;
	    int count = word_type_counts_[doc_id][d_w_id];
	    const W& word = vocab_->word( words_in_docs_[doc_id][d_w_id] );
	    for(const auto& topic_val : word_vec) {
	      writer << doc_id << "\t" << word << "\t" << t_id << "\t" << count << "\t" << topic_val << "\n";
	      ++t_id;
	    }
	    ++d_w_id;
	  }
	  ++doc_id;
	}
      }

      void learn(const LDAVStrategy& strategy, int epoch = 0,
                 isage::util::SmartWriter smart_wr = isage::util::SmartWriter("/dev/null"),
		 isage::util::SmartWriter assign_smart_wrt = isage::util::SmartWriter("/dev/null"),
		 isage::util::SmartWriter topic_smart_wr = isage::util::SmartWriter("/dev/null")) {
        int learn_iter = 0;
	bool last_iter = false;
	bool force_update = false;
	bool model_changed = false;
	while(learn_iter++ < strategy.num_learn_iters) {
 	  last_iter = learn_iter >= strategy.num_learn_iters;
          if(strategy.em_verbosity >= 0) {
            INFO << "EM step " << learn_iter;
          }
	  e_step(strategy);
	  if(! strategy.heldout) {
	    m_step(strategy);
	  }
	  if( strategy.hyper_update_iter > 0 && 
	      ( (learn_iter > strategy.hyper_update_min 
		 && learn_iter % strategy.hyper_update_iter == 0 ) 
		|| last_iter) ) {
	    update_hypers();
	    force_update = true;
	    model_changed = true;
	  }
	  if(learn_iter % strategy.update_model_every == 0 || force_update || last_iter) {
	    update_model(strategy.heldout);
	    model_changed = true;
	  }
	  if(learn_iter % strategy.print_topics_every == 0 || last_iter) {
	    std::string suff = "epoch" + std::to_string(epoch) + ".emiter" + std::to_string(learn_iter);
	    std::ostream& out_stream = topic_smart_wr.get(suff);
	    if(topic_smart_wr.to_file()) {
	      INFO << "Writing topic output to " << topic_smart_wr.name();
	    }

	    // if we haven't updated, then we should get some form of the current topics
	    if(learn_iter % strategy.update_model_every) {
	      std::vector< ModelTopicType > topics;
	      for(const auto& utopic : var_topic_params_) {
		const double norm = isage::util::sum(utopic);
		TopicType topic(isage::util::scalar_product(1.0/norm, utopic));
		topics.push_back(topic);
	      }
	      model_->print_topics(strategy.print_topics_k, *vocab_, topics);
	      if(topic_smart_wr.to_file()) {
		model_->print_topics(out_stream, *vocab_, topics);
	      }
	    } else {
	      model_->print_topics(strategy.print_topics_k, *vocab_);
	      if(topic_smart_wr.to_file()) {
		model_->print_topics(out_stream, *vocab_);
	      }
	    }
	  }
	  if( (learn_iter % strategy.print_usage_every == 0 && learn_iter > 0) || 
	      model_changed || last_iter) {
	    std::string suff = "epoch" + std::to_string(epoch) + ".emiter" + std::to_string(learn_iter);
	    std::ostream& out_stream = smart_wr.get(suff);
	    INFO << "Writing topic usage output to " << smart_wr.name();
	    model_->print_usage(out_stream);

	    ///////
	    std::string suff1 = "epoch" + std::to_string(epoch) + ".emiter" + std::to_string(learn_iter);
	    std::ostream& out_stream1 = assign_smart_wrt.get(suff1);
	    INFO << "Writing topic assignment output to " << assign_smart_wrt.name();	    
	    print_assignment_params(out_stream1);
	  }
	  update_model(strategy.heldout);
	}
      }

      void learn() {
        const LDAVStrategy strategy;
        learn(strategy);
      }

    };
  }
}

#endif
