// Header for word-based topic models

#ifndef ISAGE_WTM_SAMPLING_H_
#define ISAGE_WTM_SAMPLING_H_

#include "concrete.hpp"
#include "dmc.hpp"
#include "mathops.hpp"
#include "util.hpp"
#include "wtm.hpp"

#include <fstream>
#include <iostream>
#include "stdlib.h"
#include <time.h>

// for pair
#include "map"
#include <utility>
#include <unordered_set>
#include <string>
#include <vector>

#include <fstream>
#include <boost/algorithm/string.hpp>
#include <boost/serialization/unordered_map.hpp>
#include <boost/serialization/vector.hpp>

namespace isage { 
  namespace wtm {

    class SamplingStrategy {
    protected:
      bool heldout_ = false;
      int reestimate_usage_every_;
      int reestimate_topics_every_;
    public:
      const int num_iterations;
      const int burn_in;
      virtual bool sample_topic(int, int, int) = 0;
      virtual bool reestimate_topics(int) = 0;
      virtual bool reestimate_usage(int) = 0;
      SamplingStrategy(int num_iter, int burnin) : num_iterations((const int)num_iter), 
						   burn_in((const int)burnin) {
      }
      virtual void heldout(bool b) {
	heldout_ = b;
      }
      virtual bool heldout() {
	return heldout_;
      }
      inline const int reestimate_usage_every() {
	return reestimate_usage_every_;
      }
      inline const int reestimate_topics_every() {
	return reestimate_topics_every_;
      }
      inline void reestimate_topics_every(int i) {
	reestimate_topics_every_ = i;
      }
      inline void reestimate_usage_every(int i) {
	reestimate_usage_every_ = i;
      }
    };
    class SampleEveryIter : public SamplingStrategy {
    public:
      SampleEveryIter(int num_iter, int burnin) : SamplingStrategy(num_iter, burnin){
	this->reestimate_topics_every(100);
	this->reestimate_usage_every(100);
      }
      bool sample_topic(int i, int d, int e) {
	return true;
      }
      bool reestimate_usage(int iter_index) {
	return iter_index >= burn_in && iter_index % this->reestimate_usage_every() == 0;
      }
      bool reestimate_topics(int iter_index) {
	return iter_index >= burn_in && iter_index % this->reestimate_topics_every() == 0;
      }
    };

    /**
     * A collapsed Gibbs sampler for a discrete model.
     * This assumes that each observation is generated by a
     * categorical (discrete) distribution that is endowed
     * with a Dirichlet prior.
     */
    template <typename D, typename W>
    class CollapsedGibbsDMC {
    private:
      typedef Vocabulary<W> V;
      typedef DiscreteLDA<W, std::vector<double> > M;
      const int num_docs;
      SamplingStrategy* sample_strategy;

      std::vector< std::vector< int> >  assignments;
      // count tables
      std::vector< std::vector< int> >  c_doc_topic;
      // num_docs * num_topic;
      std::vector< std::vector< int> >  c_topic;
      // num_topics
      std::vector<int> c_topic_sums;

      std::vector<int>* c_doc_topic_ptr;

      // the model itself
      M* model_;
      Corpus<D>* corpus_;
      V* vocab_;

      int num_topics_;

      std::vector<int> num_words;

      dmc::gdmc topic_dmc;
      dmc::gdmc word_dmc;
    
      int current_word_index = -1;

    public:
      CollapsedGibbsDMC<D, W>(M* model, Corpus<D>* corpus, V* vocab) :
      num_docs(corpus->num_docs()), model_(model), corpus_(corpus), vocab_(vocab), num_topics_(model->num_topics()) {
	int num_docs = corpus->num_docs();
	for(int d = 0; d < num_docs; d++) {
	  const int nw = ((*corpus)[d]).num_words();
	  num_words.push_back(nw);
	  c_doc_topic.push_back(std::vector<int>(num_topics_));
	  assignments.push_back(std::vector<int>(nw));
	}
	const int vocab_size = vocab->num_words();
	for(int t = 0; t < model_->num_topics(); t++) {
	  c_topic_sums.push_back(0);
	  c_topic.push_back(std::vector<int>(vocab_size));
	}
	word_dmc = dmc::gdmc(vocab_size, model_->hyper_word(), model_->num_topics());
	topic_dmc = dmc::gdmc(model_->num_topics(), model_->hyper_theta(), num_docs);
      }
      ~CollapsedGibbsDMC() {
      }
      int sample_topic() {
	std::vector<double> lps(num_topics_);
	for(int topic_idx = 0; topic_idx < num_topics_; topic_idx++) {
	  double inner = word_dmc.log_u_conditional(current_word_index, c_topic[topic_idx], c_topic_sums[topic_idx], 1);
	  inner += topic_dmc.log_u_conditional(topic_idx, *c_doc_topic_ptr, num_docs, 1);
	  lps[topic_idx] = inner;
	}
	return dmc::cat::log_u_sample(lps);
      }

      template <typename TopicInitializer>
      void init(TopicInitializer ti) {
	for(int di = 0; di < num_docs; di++){
	  const int num_w = num_words[di];
	  D doc = (*corpus_)[di];
	  for(int wi = 0; wi < num_w; wi++){
	    int topic = ti(di, wi); //wi % model->num_topics();
	    assignments[di][wi] = topic;
	    ++c_doc_topic[di][topic];
	    ++c_topic[topic][ vocab_->index(doc[wi]) ];
	    ++c_topic_sums[topic];
	  }
	}
      }
      // void init() {
      // 	const int nt = num_topics_;
      // 	RandomInitializer< nt > ti;
      // 	init(ti);
      // }

      void learn(const int iter_offset = 0) {
	for(int iteration = iter_offset; iteration < sample_strategy->num_iterations + iter_offset; iteration++) {
	  for(int di = 0; di < num_docs; di++){
	    D doc = (*corpus_)[di];
	    c_doc_topic_ptr = &(c_doc_topic[di]);
	    const int num_w = num_words[di];
	    for(int wi = 0; wi < num_w; wi++){
	      if(sample_strategy->sample_topic(iteration, di, wi)) {
		//get word
		const int word = vocab_->index(doc[wi]);
		current_word_index = word;
		//remove counts
		const int prev = assignments[di][wi];
		--c_doc_topic[di][prev];
		--c_topic[prev][word];
		--c_topic_sums[prev];
		int sampled = sample_topic();
		BOOST_LOG_TRIVIAL(trace) << "sampled topic:" << sampled;
		//add counts back
		++c_topic[sampled][word];
		++c_topic_sums[sampled];
		++c_doc_topic[di][sampled];
		if(assignments[di][wi] != sampled) {
		  assignments[di][wi] = sampled;
		}
	      }
	    }
	  }
	  if(sample_strategy->reestimate_topics(iteration)) {
	    word_dmc.reestimate_collapsed_parameters(c_topic);
	  }
	  if(sample_strategy->reestimate_usage(iteration)) {
	    topic_dmc.reestimate_collapsed_parameters(c_doc_topic);
	  }
	}
      }

      // transfer the learned parameters back to the model
      void transfer_learned_parameters() {
	model_->prior_topic(topic_dmc.collapsed_params());
	model_->prior_word(word_dmc.collapsed_params());      
      }

      //setters
      void sampling_strategy(SamplingStrategy* ss) {
	sample_strategy = ss;
      }
    };
  }
}

#endif
